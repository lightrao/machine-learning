{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- Fit a line to a data set of observations\n",
    "- Use this line to predict unobserved values\n",
    "- I don’t know why they call it “regression.”\n",
    "It’s really misleading. You can use it to\n",
    "predict points in the future, the past,\n",
    "whatever. In fact time usually has nothing\n",
    "to do with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Example: Weight(kg) vs Height(cm)\n",
    "\n",
    "Linear regression is a statistical method that allows us to model the relationship between a dependent variable and one or more independent variables. It is one of the most widely used techniques for data analysis and prediction.\n",
    "\n",
    "In your example, you want to explore the relationship between weight (kg) and height (cm). Weight is the dependent variable, because it may depend on height. Height is the independent variable, because it does not depend on weight.\n",
    "\n",
    "To perform a linear regression, you need to have some data points that represent the values of weight and height for different individuals. You can plot these data points on a scatter plot, where the x-axis is height and the y-axis is weight.\n",
    "\n",
    "The goal of linear regression is to find a line that best fits the data points. This line is called the regression line, and it has the equation:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "where $y$ is the predicted value of weight, $x$ is the given value of height, $\\beta_0$ is the intercept (the value of $y$ when $x$ is zero), and $\\beta_1$ is the slope (the change in $y$ for a unit change in $x$).\n",
    "\n",
    "The regression line can be used to estimate the weight of a person given their height, or to test hypotheses about the relationship between weight and height. For example, you can test whether there is a significant positive or negative correlation between weight and height, or whether the slope of the regression line is different from zero.\n",
    "\n",
    "There are different methods to find the best-fitting regression line, such as the method of least squares, which minimizes the sum of squared errors between the observed and predicted values of weight. You can also use software tools or online calculators to perform linear regression and obtain the values of $\\beta_0$ and $\\beta_1$, as well as other statistics such as the coefficient of determination ($R^2$), which measures how well the regression line explains the variation in weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Usually using “least squares”\n",
    "\n",
    "Linear regression is a statistical method that tries to find the best-fitting line for a set of data points. The “least squares” approach is one way to measure how well a line fits the data, by minimizing the sum of the squared errors (squared errors are a way of measuring how much the predictions of a model differ from the actual values of the data. They are calculated by taking the difference between the predicted value and the actual value for each data point, and then squaring that difference.) between the observed values and the predicted values by the line. \n",
    "\n",
    "The formula for the least squares line is $y = mx + b$, where $m$ is the slope and $b$ is the y-intercept. To find $m$ and $b$, we can use these steps: \n",
    "\n",
    "- For each $(x,y)$ point, calculate $x^2$ and $xy$.\n",
    "- Sum up $x$, $y$, $x^2$ and $xy$, which gives us $\\Sigma x$, $\\Sigma y$, $\\Sigma x^2$ and $\\Sigma xy$.\n",
    "- Calculate $m$ using this formula: $m = \\frac{N\\Sigma(xy) - \\Sigma x\\Sigma y}{N\\Sigma(x^2) - (\\Sigma x)^2}$, where $N$ is the number of points.\n",
    "- Calculate $b$ using this formula: $b = \\frac{\\Sigma y - m\\Sigma x}{N}$.\n",
    "- Plug in $m$ and $b$ into the equation $y = mx + b$.\n",
    "\n",
    "Here is an example of how to use the least squares method to find the best-fitting line for some data points: \n",
    "\n",
    "| x | y | x^2 | xy |\n",
    "|---|---|-----|----|\n",
    "| 2 | 4 | 4   | 8  |\n",
    "| 3 | 5 | 9   | 15 |\n",
    "| 5 | 7 | 25  | 35 |\n",
    "| 7 | 10| 49  | 70 |\n",
    "| 9 | 15| 81  | 135|\n",
    "\n",
    "$\\Sigma x = 26$, $\\Sigma y = 41$, $\\Sigma x^2 = 168$, $\\Sigma xy = 263$, $N = 5$\n",
    "\n",
    "$m = \\frac{5 \\times 263 - 26 \\times 41}{5 \\times 168 - 26^2} = \\frac{249}{164} = 1.518$\n",
    "\n",
    "$b = \\frac{41 - 1.518 \\times 26}{5} = 0.305$\n",
    "\n",
    "$y = 1.518x + 0.305$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More than one way to do Linear Regression\n",
    "\n",
    "- Gradient Descent is an alternate method\n",
    "to least squares.\n",
    "- Basically iterates to find the line that best\n",
    "follows the contours defined by the data.\n",
    "- Can make sense when dealing with 3D\n",
    "data\n",
    "- Easy to try in Python and just compare the\n",
    "results to least squares\n",
    "    - But usually least squares is a perfectly good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's go through how to use Gradient Descent for Linear Regression in R^2 with a simple example. \n",
    "\n",
    "**Step 1: Define the Model**\n",
    "\n",
    "In R^2, a linear regression model can be represented as:\n",
    "\n",
    "y = b0 + b1*x\n",
    "\n",
    "where:\n",
    "- y is the dependent variable we want to predict.\n",
    "- x is the independent variable.\n",
    "- b0 and b1 are the parameters of the model we want to learn.\n",
    "\n",
    "**Step 2: Initialize Parameters**\n",
    "\n",
    "We initialize the parameters (b0, b1) with some random values.\n",
    "\n",
    "**Step 3: Define the Loss Function**\n",
    "\n",
    "The loss function for our linear regression model is the Mean Squared Error (MSE), defined as:\n",
    "\n",
    "MSE = 1/N * Σ(yi - (b0 + b1*xi))^2\n",
    "\n",
    "where:\n",
    "- N is the total number of observations.\n",
    "- Σ denotes the sum over all observations.\n",
    "- yi is the actual value of the dependent variable for the ith observation.\n",
    "- xi is the value of the independent variable for the ith observation.\n",
    "\n",
    "**Step 4: Compute the Gradients**\n",
    "\n",
    "The gradients of the loss function with respect to the parameters are:\n",
    "\n",
    "∂MSE/∂b0 = -2/N * Σ(yi - (b0 + b1*xi))\n",
    "\n",
    "∂MSE/∂b1 = -2/N * Σ(yi - (b0 + b1*xi)) * xi\n",
    "\n",
    "**Step 5: Update the Parameters**\n",
    "\n",
    "We update the parameters using the learning rate (α) and the gradients:\n",
    "\n",
    "b0 = b0 - α * ∂MSE/∂b0\n",
    "\n",
    "b1 = b1 - α * ∂MSE/∂b1\n",
    "\n",
    "**Step 6: Repeat Steps 4 and 5**\n",
    "\n",
    "We repeat steps 4 and 5 for a fixed number of iterations or until our loss function converges to the minimum, at this time Step-Size(α * ∂MSE/∂b0 or α * ∂MSE/∂b1) trend to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how we can fit a linear regression model (y = b0 + b1*x) to the given data using gradient descent. We'll use a learning rate of 0.01 and 1000 iterations for simplicity. We left off with our initial parameters b0 = 0 and b1 = 0, and using the Mean Squared Error (MSE) as our loss function. Our dataset is:\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "| x | y  |\n",
    "|---|----|\n",
    "| 1 | 2  |\n",
    "| 2 | 3  |\n",
    "| 3 | 4  |\n",
    "| 4 | 5  |\n",
    "| 5 | 6  |\n",
    "\n",
    "**Step 1: Initialize Parameters**\n",
    "\n",
    "We initialize b0 and b1 to be 0.\n",
    "\n",
    "So, b0 = 0 and b1 = 0.\n",
    "\n",
    "**Step 2: Define the Loss Function**\n",
    "\n",
    "The loss function is the Mean Squared Error (MSE):\n",
    "\n",
    "MSE = 1/N * Σ(yi - (b0 + b1*xi))^2\n",
    "\n",
    "**Step 3: Compute the Gradients and Update Parameters**\n",
    "\n",
    "We need to compute the gradients and update the parameters for each iteration. \n",
    "\n",
    "The gradients of the loss function with respect to the parameters are:\n",
    "\n",
    "∂MSE/∂b0 = -2/N * Σ(yi - (b0 + b1*xi))\n",
    "\n",
    "∂MSE/∂b1 = -2/N * Σ(yi - (b0 + b1*xi)) * xi\n",
    "\n",
    "***Let's calculate these for the first iteration:***\n",
    "\n",
    "For b0:\n",
    "\n",
    "∂MSE/∂b0 = -2/5 * [(2-0*1) + (3-0*2) + (4-0*3) + (5-0*4) + (6-0*5)]\n",
    "           = -4\n",
    "\n",
    "For b1:\n",
    "\n",
    "∂MSE/∂b1 = -2/5 * [(2-0*1)*1 + (3-0*2)*2 + (4-0*3)*3 + (5-0*4)*4 + (6-0*5)*5]\n",
    "           = -22\n",
    "\n",
    "Update the parameters:\n",
    "\n",
    "b0 = b0 - α * ∂MSE/∂b0 = 0 - 0.01 * -4 = 0.04\n",
    "\n",
    "b1 = b1 - α * ∂MSE/∂b1 = 0 - 0.01 * -22 = 0.22\n",
    "\n",
    "***Second Iteration:***\n",
    "\n",
    "For b0:\n",
    "\n",
    "∂MSE/∂b0 = -2/5 * [(2-(0.04+0.22*1)) + (3-(0.04+0.22*2)) + (4-(0.04+0.22*3)) + (5-(0.04+0.22*4)) + (6-(0.04+0.22*5))]\n",
    "           ≈ -3.68\n",
    "\n",
    "For b1:\n",
    "\n",
    "∂MSE/∂b1 = -2/5 * [(2-(0.04+0.22*1))*1 + (3-(0.04+0.22*2))*2 + (4-(0.04+0.22*3))*3 + (5-(0.04+0.22*4))*4 + (6-(0.04+0.22*5))*5]\n",
    "           ≈ -20.24\n",
    "\n",
    "Update the parameters:\n",
    "\n",
    "b0 = b0 - α * ∂MSE/∂b0 ≈ 0.04 - 0.01 * -3.68 ≈ 0.0768\n",
    "\n",
    "b1 = b1 - α * ∂MSE/∂b1 ≈ 0.22 - 0.01 * -20.24 ≈ 0.4244\n",
    "\n",
    "***Third Iteration:***\n",
    "\n",
    "For b0:\n",
    "\n",
    "∂MSE/∂b0 ≈ -2/5 * [(2-(0.0768+0.4244*1)) + (3-(0.0768+0.4244*2)) + (4-(0.0768+0.4244*3)) + (5-(0.0768+0.4244*4)) + (6-(0.0768+0.4244*5))]\n",
    "           ≈ -3.41\n",
    "\n",
    "For b1:\n",
    "\n",
    "∂MSE/∂b1 ≈ -2/5 * [(2-(0.0768+0.4244*1))*1 + (3-(0.0768+0.4244*2))*2 + (4-(0.0768+0.4244*3))*3 + (5-(0.0768+0.4244*4))*4 + (6-(0.0768+0.4244*5))*5]\n",
    "           ≈ -18.65\n",
    "\n",
    "Update the parameters:\n",
    "\n",
    "b0 = b0 - α * ∂MSE/∂b0 ≈ 0.0768 - 0.01 * -3.41 ≈ 0.1109\n",
    "\n",
    "b1 = b1 - α * ∂MSE/∂b1 ≈ 0.4244 - 0.01 * -18.65 ≈ 0.6109\n",
    "\n",
    "After three iterations, our parameters are approximately b0 ≈ 0.1109 and b1 ≈ 0.6109.\n",
    "\n",
    "**Step 4: Repeat Step 3**\n",
    "\n",
    "We repeat step 3 for a fixed number of iterations (in this case, 1000) or until our loss function converges to the minimum. \n",
    "\n",
    "After several iterations, you will see that the values of b0 and b1 will start to converge. These will be the parameters of the linear regression model that best fits our data.\n",
    "\n",
    "Please note that in practice, we often use libraries or built-in functions to perform these computations as they can handle more complex scenarios and are optimized for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
