{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "- Multiple regression is a statistical technique that allows us to study how more than one variable influences the one we’re interested in.\n",
    "- For example, if we want to predict the price of a car based on its many attributes (such as body style, brand, mileage, etc.), we can use multiple regression to find the relationship between the price and the attributes.\n",
    "- If we also have multiple dependent variables (things we’re trying to predict), such as the price and the fuel efficiency of a car, that’s called “multivariate regression”.\n",
    "\n",
    "# Still uses least squares\n",
    "- The basic idea of multiple regression is still to find the best-fitting line or plane that minimizes the sum of squared errors between the observed and predicted values.\n",
    "- We just end up with coefficients for each factor that tell us how much they affect the dependent variable.\n",
    "    - For example: $\\text{{price}} = \\alpha + \\beta_1 \\times \\text{{mileage}} + \\beta_2 \\times \\text{{age}} + \\beta_3 \\times \\text{{doors}}$     \n",
    "    - These coefficients imply how important each factor is (if the data is all normalized!)\n",
    "    - We can use statistical tests to determine which factors are significant and which ones can be removed from the model.\n",
    "- We can still measure the fit of the model with r-squared, which tells us how much of the variation in the dependent variable is explained by the independent variables.\n",
    "- We need to assume that the different factors are not themselves dependent on each other, which is called multicollinearity. If there is multicollinearity, the coefficients may be unreliable or unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity\n",
    "\n",
    "- Multicollinearity refers to the phenomenon in which two or more predictor variables in a regression model are highly correlated with each other. This can create issues in the model, as it violates the assumption of independence among predictors.\n",
    "\n",
    "- When multicollinearity is present, it becomes difficult to determine the individual effects of each predictor on the response variable. The coefficients of the correlated predictors can become unstable and have large standard errors, making it challenging to interpret their significance.\n",
    "\n",
    "- Multicollinearity can also lead to misleading conclusions about the relationship between predictors and the response variable. For example, it may suggest that a predictor is not statistically significant when, in reality, it is contributing to the model's predictive power.\n",
    "\n",
    "- To detect multicollinearity, common methods include examining correlation matrices, calculating variance inflation factors (VIF), and performing eigenvalue analysis. If multicollinearity is found, there are several techniques to address it, such as removing one or more correlated predictors, combining them into a single variable, or using dimensionality reduction techniques like principal component analysis (PCA).\n",
    "\n",
    "- Addressing multicollinearity is important for building robust regression models and obtaining reliable results. By understanding and managing multicollinearity, researchers can improve the accuracy and interpretability of their models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
