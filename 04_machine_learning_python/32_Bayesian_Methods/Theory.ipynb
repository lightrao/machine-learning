{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods\n",
    "\n",
    "- $P(A\\vert B)={\\frac{P(A)P(B\\vert A)}{P(B)}}$\n",
    "- Let’s use it for machine learning! I want a spam classifier.\n",
    "- Example: how would we express the probability of an email being spam if it contains the word “Free”?\n",
    "- $P(Spam\\vert Free)={\\frac{P(Spam)P(Free\\vert Spam)}{P(Free)}}$ \n",
    "- The numerator is the probability of a message being spam and containing the word “Free” (this is subtly different from what we’re looking for)\n",
    "- The denominator is the overall probability of an email containing the word “Free”.\n",
    "    - Equivalent to $P(Free\\vert Spam)P(Spam) + P(Free\\vert Not Spam)P(Not Spam)$\n",
    "- So together – this ratio is the % of emails with the word “Free” that are spam.\n",
    "\n",
    "# What about all the other words?\n",
    "\n",
    "- We can construct P(Spam | Word) for every (meaningful) word we encounter during training\n",
    "- Then multiply these together when analyzing a new email to get the probability of it being spam.\n",
    "- Assumes the presence of different words are independent of each other – one reason this is called “Naïve Bayes”.\n",
    "\n",
    "# Sounds like a lot of work\n",
    "\n",
    "- Scikit-learn to the rescue!\n",
    "- The CountVectorizer lets us operate on lots of words at once, and MultinomialNB does all the heavy lifting on Naïve Bayes.\n",
    "- We’ll train it on known sets of spam and “ham” (non-spam) emails\n",
    "    - So this is supervised learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Multinomial Naive Bayes Classification) Example\n",
    "\n",
    "Suppose my inbox has:\n",
    "- normal emails: 8\n",
    "- spam emails: 4\n",
    "\n",
    "Then, the prior probabilities are:\n",
    "- $p(\\text{Normal}) = \\frac{8}{8 + 4} = 0.67$\n",
    "- $p(\\text{Spam}) = \\frac{4}{8 + 4} = 0.33$\n",
    "\n",
    "My normal emails include the following words and their frequencies:\n",
    "- Dear: 8\n",
    "- Friend: 5\n",
    "- Lunch: 3\n",
    "- Money: 1\n",
    "\n",
    "The total number of words in normal emails is 17. Therefore, the conditional probabilities are:\n",
    "- $p(\\text{Dear}|\\text{Normal}) = \\frac{8}{17} = 0.47$\n",
    "- $p(\\text{Friend}|\\text{Normal}) = \\frac{5}{17} = 0.29$\n",
    "- $p(\\text{Lunch}|\\text{Normal}) = \\frac{3}{17} = 0.18$\n",
    "- $p(\\text{Money}|\\text{Normal}) = \\frac{1}{17} = 0.06$\n",
    "\n",
    "My spam emails include the following words and their frequencies:\n",
    "- Dear: 2\n",
    "- Friend: 1\n",
    "- Lunch: 0\n",
    "- Money: 4\n",
    "\n",
    "The total number of words in spam emails is 7. Therefore, the conditional probabilities are:\n",
    "- $p(\\text{Dear}|\\text{Spam}) = \\frac{2}{7} = 0.29$\n",
    "- $p(\\text{Friend}|\\text{Spam}) = \\frac{1}{7} = 0.14$\n",
    "- $p(\\text{Lunch}|\\text{Spam}) = \\frac{0}{7} = 0$\n",
    "- $p(\\text{Money}|\\text{Spam}) = \\frac{4}{7} = 0.57$\n",
    "\n",
    "I receive a new email that includes:\n",
    "- Dear: 1\n",
    "- Friend: 1\n",
    "\n",
    "Is it a normal email or a spam email?\n",
    "\n",
    "Using the naive Bayes formula, I can calculate the posterior probabilities as follows:\n",
    "\n",
    "- $p(\\text{Normal}|\\text{Dear Friend}) = p(\\text{Normal}) * p(\\text{Dear}|\\text{Normal}) * p(\\text{Friend}|\\text{Normal}) = 0.67 * 0.47 * 0.29 = 0.09$\n",
    "- $p(\\text{Spam}|\\text{Dear Friend}) = p(\\text{Spam}) * p(\\text{Dear}|\\text{Spam}) * p(\\text{Friend}|\\text{Spam}) = 0.33 * 0.29 * 0.14 = 0.01$\n",
    "\n",
    "Since $p(\\text{Normal}|\\text{Dear Friend}) > p(\\text{Spam}|\\text{Dear Friend})$, I can conclude that the email is **normal**.\n",
    "\n",
    "I receive another email that includes:\n",
    "- Lunch: 1\n",
    "- Money: 4\n",
    "\n",
    "Is it a normal email or a spam email?\n",
    "\n",
    "Using the naive Bayes formula, I can calculate the posterior probabilities as follows:\n",
    "\n",
    "- $p(\\text{Normal}|\\text{Lunch Money Money Money Money}) = p(\\text{Normal}) * p(\\text{Lunch}|\\text{Normal}) * p(\\text{Money}|\\text{Normal}) ^ 4 = 0.67 * 0.18 * 0.06 ^ 4 = 0.000002$\n",
    "- $p(\\text{Spam}|\\text{Lunch Money Money Money Money}) = p(\\text{Spam}) * p(\\text{Lunch}|\\text{Spam}) * p(\\text{Money}|\\text{Spam}) ^ 4 = 0.33 * 0 * 0.57 ^ 4 = 0$\n",
    "\n",
    "Since $p(\\text{Normal}|\\text{Lunch Money Money Money Money}) > p(\\text{Spam}|\\text{Lunch Money Money Money Money})$, I can conclude that the email is **normal**. However, this is **wrong**. The email is clearly a spam email, but the naive Bayes classifier fails to detect it because of the zero probability problem. This happens when one of the conditional probabilities is zero, which makes the whole posterior probability zero regardless of the other factors.\n",
    "\n",
    "To fix this problem, I can use a technique called **Laplace smoothing**, which adds a small constant (usually 1) to each word's frequency to avoid zero probabilities. This also helps to account for words that may not appear in the training data but may appear in the test data.\n",
    "\n",
    "Using Laplace smoothing with alpha = 1, I can calculate the new conditional probabilities as follows:\n",
    "\n",
    "- $p(\\text{Lunch}|\\text{Spam}) = \\frac{0 + 1}{7 + 4} = 0.07$\n",
    "- $p(\\text{Lunch}|\\text{Normal}) = \\frac{3 + 1}{17 + 4} = 0.19$\n",
    "- $p(\\text{Money}|\\text{Spam}) = \\frac{4 + 1}{7 + 4} = 0.45$\n",
    "- $p(\\text{Money}|\\text{Normal}) = \\frac{1 + 1}{17 + 4} = 0.10$\n",
    "\n",
    "Using the naive Bayes formula with the smoothed probabilities, I can calculate the new posterior probabilities as follows:\n",
    "\n",
    "- $p(\\text{Normal}|\\text{Lunch Money Money Money Money}) = p(\\text{Normal}) * p(\\text{Lunch}|\\text{Normal}) * p(\\text{Money}|\\text{Normal}) ^ 4 = 0.67 * 0.19 * 0.10 ^ 4 = 0.00001$\n",
    "- $p(\\text{Spam}|\\text{Lunch Money Money Money Money}) = p(\\text{Spam}) * p(\\text{Lunch}|\\text{Spam}) * p(\\text{Money}|\\text{Spam}) ^ 4 = 0.33 * 0.07 * 0.45 ^ 4 = 0.00094$\n",
    "\n",
    "Now, $p(\\text{Spam}|\\text{Lunch Money Money Money Money}) > p(\\text{Normal}|\\text{Lunch Money Money Money Money})$, which is the correct result. The email is **spam**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightTf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
