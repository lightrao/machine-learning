{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy (for data science) \n",
    "\n",
    "Entropy is a measure of the **uncertainty** or **randomness** of a system(A measure of a data set’s disorder – how same or different it is). It is often used in data science to quantify the **information content** or **information gain** of a variable, a distribution, or a model.\n",
    "\n",
    "If we classify a data set into N different classes (example: a data set of animal attributes and their species)\n",
    "- The entropy is 0 if all of the classes in the data are the same (everyone is an iguana)\n",
    "- The entropy is high if they’re all different\n",
    "\n",
    "One way to define entropy is using the concept of **probability**. If $p(x)$ is the probability of observing a value $x$ in a system, then the entropy $H(S)$ of the system is given by:\n",
    "\n",
    "$$H(S) = -\\sum_{x} p(x) \\log p(x)$$\n",
    "\n",
    "The logarithm can be taken with any base, but a common choice is base 2, which means the entropy is measured in **bits**. The entropy is always non-negative, and it is zero if and only if the system has only one possible state (i.e., no uncertainty).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy example\n",
    "\n",
    "1. There are 2 chickens and 8 rabbits in the cage. In this case, the probability of observing a chicken is $p(c) = 2/10 = 0.2$, and the probability of observing a rabbit is $p(r) = 8/10 = 0.8$. Using the formula for entropy, we get:\n",
    "\n",
    "$$H(S) = -\\sum_{x} p(x) \\log p(x)$$\n",
    "$$H(S) = -[p(c) \\log p(c) + p(r) \\log p(r)]$$\n",
    "$$H(S) = -[0.2 \\log 0.2 + 0.8 \\log 0.8]$$\n",
    "$$H(S) \\approx 0.7219 \\text{ bits}$$\n",
    "\n",
    "2. There are 8 chickens and 2 rabbits in the cage. In this case, the probability of observing a chicken is $p(c) = 8/10 = 0.8$, and the probability of observing a rabbit is $p(r) = 2/10 = 0.2$. Using the formula for entropy, we get:\n",
    "\n",
    "$$H(S) = -\\sum_{x} p(x) \\log p(x)$$\n",
    "$$H(S) = -[p(c) \\log p(c) + p(r) \\log p(r)]$$\n",
    "$$H(S) = -[0.8 \\log 0.8 + 0.2 \\log 0.2]$$\n",
    "$$H(S) \\approx 0.7219 \\text{ bits}$$\n",
    "\n",
    "3. There are 5 chickens and 5 rabbits in the cage. In this case, the probability of observing a chicken is $p(c) = 5/10 = 0.5$, and the probability of observing a rabbit is $p(r) = 5/10 = 0.5$. Using the formula for entropy, we get:\n",
    "\n",
    "$$H(S) = -\\sum_{x} p(x) \\log p(x)$$\n",
    "$$H(S) = -[p(c) \\log p(c) + p(r) \\log p(r)]$$\n",
    "$$H(S) = -[0.5 \\log 0.5 + 0.5 \\log 0.5]$$\n",
    "$$H(S) = 1 \\text{ bit}$$\n",
    "\n",
    "4. There are 10 chickens and 0 rabbits in the cage. In this case, the probability of observing a chicken is $p(c) = 10/10 = 1$, and the probability of observing a rabbit is $p(r) = 0/10 = 0$. Using the formula for entropy, we get:\n",
    "\n",
    "$$H(S) = -\\sum_{x} p(x) \\log p(x)$$\n",
    "$$H(S) = -[p(c) \\log p(c) + p(r) \\log p(r)]$$\n",
    "$$H(S) = -[1 \\log 1 + 0 \\log 0]$$\n",
    "$$H(S) = 0 \\text{ bits}$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
