{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees\n",
    "    Creat(Train): Gini Impurity\n",
    "    Use: ID3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How Decision Trees Work\n",
    "• At each step, find the attribute we can use to partition the data set to\n",
    "minimize the entropy of the data at the next step\n",
    "• Fancy term for this simple algorithm: \n",
    "• It is a greedy algorithm – as it goes down the tree, it just picks the\n",
    "decision that reduce entropy the most at that stage.\n",
    "▫ That might not actually result in an optimal tree.\n",
    "▫ But it works.\n",
    "\n",
    "Random Forests\n",
    "• Decision trees are very susceptible to\n",
    "overfitting\n",
    "• To fight this, we can construct several\n",
    "alternate decision trees and let them “vote”\n",
    "on the final classification\n",
    "▫ Randomly re-sample the input data for each\n",
    "tree (fancy term for this: bootstrap\n",
    "aggregating or bagging)\n",
    "▫ Randomize a subset of the attributes each\n",
    "step is allowed to choose from"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
