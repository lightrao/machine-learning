{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using XGBoost is easy. Maybe too easy, considering it's generally considered the best ML algorithm around right now.\n",
    "\n",
    "To install it, just:\n",
    "\n",
    "pip install xgboost\n",
    "\n",
    "Let's experiment using the Iris data set. This data set includes the width and length of the petals and sepals of many Iris flowers, and the specific species of Iris the flower belongs to. Our challenge is to predict the species of a flower sample just based on the sizes of its petals. We'll revisit this data set later when we talk about principal component analysis too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "4\n",
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Importing the 'load_iris' function from the 'sklearn.datasets' module.\n",
    "# This function is used to load the Iris dataset, a classic dataset in machine learning and statistics.\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Calling the 'load_iris' function to load the Iris dataset.\n",
    "# The function returns an object that contains the dataset along with additional information.\n",
    "iris = load_iris()\n",
    "\n",
    "# The Iris dataset is stored in a structured format where 'iris.data' contains the data points.\n",
    "# Each data point is a feature vector representing an Iris flower.\n",
    "# 'iris.data.shape' returns a tuple where the first element is the number of samples (flowers)\n",
    "# and the second element is the number of features (characteristics of each flower) in the dataset.\n",
    "numSamples, numFeatures = iris.data.shape\n",
    "\n",
    "# Printing the number of samples in the Iris dataset.\n",
    "# This number represents how many individual flowers are included in the dataset.\n",
    "print(numSamples)\n",
    "\n",
    "# Printing the number of features in the Iris dataset.\n",
    "# Each feature represents a specific measurement or characteristic of the Iris flowers (petal length, petal width, sepal Length, sepal Width).\n",
    "print(numFeatures)\n",
    "\n",
    "# The Iris dataset includes labels for each data point, categorizing them into different types of Iris flowers.\n",
    "# 'iris.target_names' contains the names of these categories or species.\n",
    "# Here, we convert this array into a list for easier reading and print it out.\n",
    "# This shows the different types of Iris flowers that the dataset classifies.\n",
    "print(list(iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide our data into 20% reserved for testing our model, and the remaining 80% to train it with. By withholding our test data, we can make sure we're evaluating its results based on new flowers it hasn't seen before. Typically we refer to our features (in this case, the petal sizes) as X, and the labels (in this case, the species) as y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train_test_split function from the sklearn.model_selection module.\n",
    "# This function is used for splitting data arrays into two subsets: for training data and for testing data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and testing subsets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data,  # The feature vectors (independent variables) from the Iris dataset.\n",
    "    iris.target,  # The labels (dependent variable) for each feature vector.\n",
    "    test_size=0.2,  # Allocating 20% of the dataset for testing and the rest for training.\n",
    "    random_state=0,  # Setting a seed for the random number generator for reproducibility.\n",
    ")\n",
    "\n",
    "# After this split:\n",
    "# X_train contains the feature vectors for training the model.\n",
    "# y_train contains the corresponding labels for X_train.\n",
    "# X_test contains the feature vectors for testing the model.\n",
    "# y_test contains the corresponding labels for X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load up XGBoost, and convert our data into the DMatrix format it expects. One for the training data, and one for the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the xgboost library. XGBoost stands for eXtreme Gradient Boosting\n",
    "# and is an efficient implementation of gradient boosted decision trees.\n",
    "import xgboost as xgb\n",
    "\n",
    "# Preparing the training data for XGBoost.\n",
    "# The DMatrix is a data structure unique to XGBoost that optimizes both memory efficiency\n",
    "# and training speed, which is especially beneficial for large datasets.\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "# X_train: The feature vectors used for training the model.\n",
    "# label=y_train: The labels corresponding to each training feature vector.\n",
    "\n",
    "# Similarly, preparing the testing data for XGBoost.\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "# X_test: The feature vectors used for testing the model.\n",
    "# label=y_test: The labels corresponding to each testing feature vector.\n",
    "\n",
    "# After executing this code, you'll have 'train' and 'test' datasets in a format\n",
    "# that can be efficiently utilized by XGBoost for model training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our hyperparameters. We're choosing softmax since this is a multiple classification problem, but the other parameters should ideally be tuned through experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters for the XGBoost model in a dictionary named 'param'.\n",
    "param = {\n",
    "    \"max_depth\": 4,  # 'max_depth' specifies the maximum depth of a tree. It controls over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "    \"eta\": 0.3,  # 'eta' is the learning rate. It makes the model more robust by shrinking the weights on each step, preventing overfitting.\n",
    "    \"objective\": \"multi:softmax\",  # 'objective' specifies the learning task and the corresponding learning objective. Here, 'multi:softmax' is used for multiclass classification, and it returns predicted class labels (not probabilities).\n",
    "    \"num_class\": 3,  # 'num_class' is set to 3, as there are three classes in the Iris dataset (setosa, versicolor, virginica).\n",
    "}\n",
    "\n",
    "# Setting the number of training iterations.\n",
    "epochs = 10  # 'epochs' (also known as num_boost_round in XGBoost) represents the number of times the boosting process is to be run. It's akin to the number of training epochs in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and train our model using these parameters as a first guess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the XGBoost model using the previously defined parameters and training dataset.\n",
    "model = xgb.train(\n",
    "    param,  # The dictionary of hyperparameters defined earlier.\n",
    "    train,  # The DMatrix containing the training data (feature vectors and labels).\n",
    "    epochs,  # The number of training iterations (boosting rounds).\n",
    ")\n",
    "\n",
    "# After executing this line, 'model' will be an XGBoost model trained on your data\n",
    "# according to the specified hyperparameters and for the defined number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the trained model to predict classifications for the data we set aside for testing. Each classification number we get back corresponds to a specific species of Iris.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the trained XGBoost model on the test dataset.\n",
    "predictions = model.predict(test)\n",
    "# 'test' is the DMatrix containing the test data (feature vectors).\n",
    "\n",
    "# The variable 'predictions' will now contain the predicted labels for each sample in the test dataset.\n",
    "# These predictions can be compared against the actual labels ('y_test') to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 0. 0.\n",
      " 2. 0. 0. 1. 1. 0.]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# This line will display the array of predicted labels for each sample in the test dataset. Each element in this array corresponds to\n",
    "# the predicted class for a test sample, as determined by the model.\n",
    "print(predictions)\n",
    "# This line will print the total number of predictions made, which should be equal to the number of samples in the test dataset.\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the accuracy on the test data...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the accuracy_score function from sklearn.metrics.\n",
    "# This function is used to compute the accuracy of a classification model.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculating the accuracy of the model.\n",
    "# Accuracy is the ratio of correctly predicted observations to the total observations.\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "# y_test: The actual labels of the test dataset.\n",
    "# predictions: The predicted labels generated by the XGBoost model.\n",
    "\n",
    "# The variable 'accuracy' now holds the accuracy score of the model.\n",
    "# It represents the proportion of correct predictions in all predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy crow! It's perfect, and that's just with us guessing as to the best hyperparameters!\n",
    "\n",
    "Normally I'd have you experiment to find better hyperparameters as an activity, but you can't improve on those results. Instead, see what it takes to make the results worse! How few epochs (iterations) can I get away with? How low can I set the max_depth? Basically try to optimize the simplicity and performance of the model, now that you already have perfect accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
